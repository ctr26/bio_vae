{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977feac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchdata.datapipes as dp\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "from PIL import Image\n",
    "import io\n",
    "from joblib import Memory\n",
    "from PIL import UnidentifiedImageError\n",
    "import bioimage_embed.config as config\n",
    "from hydra.utils import instantiate\n",
    "import os\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "# import fsspec\n",
    "import submitit\n",
    "import bioimage_embed\n",
    "\n",
    "# import submitit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location=\".\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FTP host and root directory\n",
    "host = \"ftp.ebi.ac.uk\"\n",
    "root = \"pub/databases/IDR\"\n",
    "dataset = \"idr0093-mueller-perturbation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a60ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup fsspec filesystem for FTP access\n",
    "# fs = fsspec.filesystem(\"ftp\", host=host, anon=True)\n",
    "fs = fsspec.filesystem(\"ftp\", host=host, anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657c25f",
   "metadata": {},
   "source": [
    "# Glob pattern to match the files you're interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb1331",
   "metadata": {},
   "source": [
    "glob_str = f\"{root}/{dataset}/**/\"\n",
    "folders = fs.glob(glob_str, recursive=True)\n",
    "dp = IterableWrapper(folders).list_files_by_fsspec(\n",
    "    anon=True,\n",
    "    protocol=\"ftp\",\n",
    "    host=host,\n",
    "    recursive=True,\n",
    "    masks=[\"*.tif\", \"*.tiff\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_str = f\"{root}/{dataset}/**/*.tif*\"\n",
    "NUM_GPUS_PER_NODE = 1\n",
    "NUM_NODES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10741f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_file_list(glob_str, fs):\n",
    "    return fs.glob(glob_str, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7794405",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_file_list(glob_str, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46438ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(x):\n",
    "    try:\n",
    "        # Attempt to open the image\n",
    "        print(x[0])\n",
    "        stream = x[1].read()\n",
    "        print(\"Valid file\")\n",
    "        return stream\n",
    "    except Exception:\n",
    "        print(\"Invalid file\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(x):\n",
    "    return Image.open(io.BytesIO(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(x):\n",
    "    try:\n",
    "        # Attempt to open the image\n",
    "        image = read_image(x)\n",
    "        image.verify()  # Ensure it's a valid image\n",
    "        print(\"Valid image\")\n",
    "        return True\n",
    "    except (IOError, UnidentifiedImageError):\n",
    "        print(\"Invalid image\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = (\n",
    "    # IterableWrapper(files)\n",
    "    IterableWrapper(files)\n",
    "    .open_files_by_fsspec(\n",
    "        anon=True,\n",
    "        protocol=\"ftp\",\n",
    "        host=host,\n",
    "        mode=\"rb\",\n",
    "        filecache={\"cache_storage\": \"tmp/idr\"},\n",
    "    )\n",
    "    # .filter(filter_fn=is_valid_file)\n",
    "    .map(read_file)\n",
    "    .filter(filter_fn=is_valid_image)\n",
    "    .map(lambda x: Image.open(io.BytesIO(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(dp))\n",
    "print(a)\n",
    "\n",
    "\n",
    "def train(num_gpus_per_node=1, num_nodes=1):\n",
    "    # Define FTP host and root directory\n",
    "    host = \"ftp.ebi.ac.uk\"\n",
    "    root = \"pub/databases/IDR\"\n",
    "    dataset = \"idr0093-mueller-perturbation\"\n",
    "\n",
    "    # # Setup fsspec filesystem for FTP access\n",
    "    # fs = fsspec.filesystem(\"ftp\", host=host, anon=True)\n",
    "    fs = fsspec.filesystem(\"ftp\", host=host, anon=True)\n",
    "\n",
    "    glob_str = f\"{root}/{dataset}/**/*.tif*\"\n",
    "\n",
    "    files = get_file_list(glob_str, fs)\n",
    "\n",
    "    transform = instantiate(config.Transform())\n",
    "\n",
    "    dataset = (\n",
    "        # IterableWrapper(files)\n",
    "        IterableWrapper(files)\n",
    "        .open_files_by_fsspec(\n",
    "            anon=True,\n",
    "            protocol=\"ftp\",\n",
    "            host=host,\n",
    "            mode=\"rb\",\n",
    "            filecache={\"cache_storage\": \"tmp/idr\"},\n",
    "        )\n",
    "        # .filter(filter_fn=is_valid_file)\n",
    "        .map(read_file)\n",
    "        .filter(filter_fn=is_valid_image)\n",
    "        .map(lambda x: Image.open(io.BytesIO(x)))\n",
    "        .map(lambda x: x.convert(\"RGB\"))\n",
    "        .map(transform)\n",
    "        .set_length(len(files))\n",
    "        # TODO add zip_with_iter() to combine the image and the label\n",
    "        # .zip_with_iter()\n",
    "    )\n",
    "\n",
    "    # dataset = datasets.ImageFolder(transform=transform)\n",
    "\n",
    "    a = next(iter(dataset))\n",
    "    print(a)\n",
    "\n",
    "    # dp = Mapper(dp, lambda x: x.read())\n",
    "    # dp = Mapper(dp, lambda x: Image.open(io.BytesIO(x)))\n",
    "    # next(iter(dp))\n",
    "    # files = fs.glob(glob_str,recursive=True)\n",
    "    # print(files)\n",
    "    # # Use FSSpecFileLister to list files from the FTP server\n",
    "    # # lister_dp = FSSpecFileLister(root=f\"ftp://{host}\",\n",
    "    # #     anon=True, protocol=\"ftp\", host=host\n",
    "    # # )\n",
    "\n",
    "    # lister_dp = FSSpecFileLister(root=files[0],\n",
    "    #     anon=True, protocol=\"ftp\", host=host\n",
    "    # )\n",
    "\n",
    "    # # Open the listed files using FSSpecFileOpener\n",
    "\n",
    "    # file_opener_dp = FSSpecFileOpener(lister_dp, mode=\"rb\")\n",
    "\n",
    "    # first_file = [\n",
    "    #     f\"pub/databases/IDR/idr0093-mueller-perturbation/20200728-ftp/001_B02_T0001F001L01A01Z01C01.tif\"\n",
    "    # ]\n",
    "\n",
    "    # for i, img in enumerate(dp):\n",
    "    #     print(img)\n",
    "    # # %% [markdown]\n",
    "    # # root = \"/nfs/ftp/public/databases/IDR/\"\n",
    "    # # ftp = \"ftp://ftp.ebi.ac.uk/pub/databases/idr/\"\n",
    "\n",
    "    # import wandb\n",
    "    # from pytorch_lightning import LightningModule, Trainer\n",
    "    # import albumentations as A\n",
    "    # from types import SimpleNamespace\n",
    "    # from ray import tune\n",
    "    # import numpy as np\n",
    "    # from ray.train.torch import TorchTrainer\n",
    "    # from ray.train import ScalingConfig\n",
    "    # from hydra.utils import instantiate\n",
    "    # import os\n",
    "    # import glob\n",
    "    # from PIL import Image\n",
    "    # from typing import List\n",
    "    # from torch.utils.data import Dataset\n",
    "    # import torch\n",
    "    # from joblib import Memory\n",
    "    # from pydantic.dataclasses import dataclass\n",
    "    # from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "    # import os\n",
    "    # import fsspec\n",
    "    # from torchdata.datapipes.iter import FSSpecFileLister, FSSpecFileOpener\n",
    "    # import torchdata.datapipes as dp\n",
    "\n",
    "    # # \"https://ftp.ebi.ac.uk/pub/databases/IDR/\"\n",
    "    # host = \"ftp.ebi.ac.uk\"\n",
    "    # root = \"pub/databases/IDR\"\n",
    "    # dataset = \"idr0093-mueller-perturbation\"\n",
    "    # fs = fsspec.filesystem('ftp', host=host, anon=True)\n",
    "    # glob_str = f\"{root}/{dataset}/**/*.tif*\"\n",
    "    # from torchdata.datapipes.iter import FSSpecFileLister\n",
    "    # # lister = FSSpecFileLister(root=root, fs=fs, masks=glob_str)\n",
    "    # from torchdata.datapipes.iter import IterableWrapper, Mapper,MapDataPipe\n",
    "\n",
    "    # # file_paths_dp = IterableWrapper(lister,ftp=)\n",
    "    # file_opener_dp = FSSpecFileOpener(file_paths_dp,ftp={\"host\"=host, \"anon\"=True})\n",
    "\n",
    "    # dp = IterableWrapper([\"ftp://BUCKET_NAME\"]).list_files_by_fsspec()\n",
    "\n",
    "    # files = fs.glob(glob_str,recursive=True)\n",
    "\n",
    "    # # fs.ls(f\"{root}{dataset}\")\n",
    "    # # 56008 files:\n",
    "    # files\n",
    "\n",
    "    # # %% [markdown]\n",
    "    # # dataset = datasets.ImageFolder(transform=transform)\n",
    "\n",
    "    params = {\n",
    "        \"model\": \"resnet50_vqvae\",\n",
    "        # \"data\": \"data\",\n",
    "        \"opt\": \"lamb\",\n",
    "        \"latent_dim\": 224**2 // 4,\n",
    "        \"max_epochs\": 1000,\n",
    "        \"max_steps\": -1,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"momentum\": 0.9,\n",
    "        # \"sched\": \"cosine\",\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 16,\n",
    "        \"sched\": \"cosine\",\n",
    "    }\n",
    "    # memory = Memory(location='.', verbose=0)\n",
    "\n",
    "    # @memory.cache\n",
    "    # def get_file_list(glob_str,fs):\n",
    "    #     # return fs.glob(glob_str)\n",
    "    #     return fs.open(glob_str,filecache={'cache_storage':'tmp/idr'})\n",
    "    #     # return fsspec.open_files(glob_str, recursive=True)\n",
    "    #     # return glob.glob(os.path.join(glob_str), recursive=True)\n",
    "\n",
    "    # # def collate_fn(batch):\n",
    "    # #     # Filter out None values\n",
    "    # #     batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    # #     return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "    # # class GlobDataset(Dataset):\n",
    "    # #     def __init__(self, glob_str,transform=None,fs=fsspec.filesystem('file')):\n",
    "    # #         print(\"Getting file list, this may take a while\")\n",
    "    # #         self.file_list = get_file_list(glob_str,fs)\n",
    "    # #         print(\"Done getting file list\")\n",
    "    # #         self.transform = transform\n",
    "\n",
    "    # #     def __len__(self):\n",
    "    # #         return len(self.file_list)\n",
    "\n",
    "    # #     def __getitem__(self, idx):\n",
    "    # #         if torch.is_tensor(idx):\n",
    "    # #             idx = idx.tolist()\n",
    "\n",
    "    # #         img_name = self.file_list[idx]\n",
    "    # #         obj = fs.open(img_name,filecache={'cache_storage':'tmp/idr'})\n",
    "    # #         try:\n",
    "    # #             with obj as f:\n",
    "    # #                 image = Image.open(f)\n",
    "    # #             # image = Image.open(img_name)\n",
    "    # #         except:\n",
    "    # #             return None,None\n",
    "    # #         # breakpoint()\n",
    "    # #         image = np.array(image)\n",
    "    # #         if self.transform:\n",
    "    # #             # t = A.Compose([A.ToRGB(),transform, A.RandomCrop(224,224)])\n",
    "    # #             t = A.Compose([A.ToRGB(),self.transform])\n",
    "    # #             image = t(image=image)\n",
    "\n",
    "    # #         # breakpoint()\n",
    "\n",
    "    # #         return image[\"image\"], 0\n",
    "\n",
    "    # # root_dir = '/nfs/research/uhlmann/ctr26/idr/idr0093-mueller-perturbation/'\n",
    "    # # fs = fsspec.filesystem('file')\n",
    "    # # fs = fsspec.filesystem(\n",
    "    # #         'ftp', host='ftp.ebi.ac.uk',\n",
    "    # #         cache_storage='/tmp/files/')\n",
    "    # # root_dir = '/pub/databases/IDR/idr0093-mueller-perturbation/'\n",
    "\n",
    "    # # # /nfs/ftp/public/databases/IDR/idr0093-mueller-perturbation/'\n",
    "    # # # /nfs/ftp/public/databases/IDR/\n",
    "    print(\"training\")\n",
    "    input_dim = [3, 224, 224]\n",
    "\n",
    "    # mock_dataset = config.ImageFolderDataset(\n",
    "    #     _target_=\"bioimage_embed.datasets.FakeImageFolder\",\n",
    "    #     image_size=input_dim,\n",
    "    #     num_classes=1,\n",
    "    # )\n",
    "\n",
    "    # transform = instantiate(config.ATransform())\n",
    "    # dataset = GlobDataset(root_dir+'**/*.tif*',transform,fs=fs)\n",
    "    # dataset = RandomDataset(32, 64)\n",
    "    dataloader = config.DataLoader(dataset=dataset, num_workers=os.cpu_count())\n",
    "\n",
    "    assert instantiate(dataloader, batch_size=1)\n",
    "    # assert dataset[0]\n",
    "\n",
    "    model = config.Model(input_dim=input_dim)\n",
    "\n",
    "    lit_model = config.LightningModel(model=model)\n",
    "    wandb = pl_loggers.WandbLogger(project=\"idr\", name=\"0093\", log_model=\"all\")\n",
    "    trainer = config.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=num_gpus_per_node,\n",
    "        num_nodes=num_nodes,\n",
    "        strategy=\"ddp\",\n",
    "        callbacks=[],\n",
    "        # plugin=[],\n",
    "        logger=[wandb],\n",
    "    )\n",
    "\n",
    "    cfg = config.Config(\n",
    "        dataloader=dataloader,\n",
    "        lit_model=lit_model,\n",
    "        trainer=trainer,\n",
    "        recipe=config.Recipe(**params),\n",
    "    )\n",
    "    # breakpoint()\n",
    "\n",
    "    bie = bioimage_embed.BioImageEmbed(cfg)\n",
    "    # wandb.watch(bie.icfg.lit_model, log=\"all\")\n",
    "    # wandb.run.define_metric(\"mse/val\", summary=\"best\")\n",
    "    # wandb.run.define_metric(\"loss/val.loss\", summary=\"best\")\n",
    "\n",
    "    bie.train()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "def main():\n",
    "    logdir = \"lightning_slurm/\"\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # executor is the submission interface (logs are dumped in the folder)\n",
    "    executor = submitit.AutoExecutor(folder=logdir)\n",
    "    executor.update_parameters(\n",
    "        mem_gb=2 * 32 * 4,  # 2GB per CPU, 32 CPUs per task, 4 tasks per node\n",
    "        timeout_min=1440 * 2,  # 48 hours\n",
    "        # slurm_partition=\"your_partition_name\",  # Replace with your partition name\n",
    "        gpus_per_node=NUM_GPUS_PER_NODE,\n",
    "        tasks_per_node=1,\n",
    "        cpus_per_task=8,\n",
    "        nodes=NUM_NODES,\n",
    "        slurm_constraint=\"a100\",\n",
    "    )\n",
    "    job = executor.submit(train, NUM_GPUS_PER_NODE, NUM_NODES)\n",
    "    print(job)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
